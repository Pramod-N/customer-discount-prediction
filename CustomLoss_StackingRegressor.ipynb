{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomLoss-StackingRegressor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPXDRRPaWvLjFFKQGO9PGpV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afn212YmSyBO",
        "outputId": "9c5cf6b0-8a0c-49bc-c588-47f870f3e58a"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import requests, zipfile, io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:17: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  \"Since version 1.0, \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYfu5ZWlTJ9f"
      },
      "source": [
        "#convert the datetime column to number of days since previous order\n",
        "train_df['Last_order_placed_date'] = pd.to_datetime(train_df['Last_order_placed_date'])\n",
        "\n",
        "train_df['Days_since_last_order'] = (datetime.datetime.now() - train_df['Last_order_placed_date']).dt.days"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ONaZUObTPIQ"
      },
      "source": [
        "#drop columns \n",
        "train_df = train_df.drop(columns=['Last_order_placed_date','Customer_ID','No_of_issues_raised'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X060MhYuTRlY"
      },
      "source": [
        "conversion_names = {\n",
        "          \"Category_of_customers\" : {\"Active\" : 2, \"Inactive\":0,\"Passive\":1},\n",
        "          \"Premium_membership\" : {\"Yes\" : 1, \"No\":0}\n",
        "}\n",
        "train_df.replace(conversion_names,inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX25PxZ9TV7Q"
      },
      "source": [
        "X = train_df.drop('Discount_percentage',axis=1)\n",
        "y = train_df['Discount_percentage']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjRWMsztTXwx"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxgbparTekg"
      },
      "source": [
        "## Custom Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xscsr3bThE4"
      },
      "source": [
        "In the below set of codes, i write custom loss function for XGBoost and LightGBM models. I apply weighted loss function by dividing the data into bins based on target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eELu6ZABT-T4"
      },
      "source": [
        "def calcweights(dtrain):\n",
        "    weights = np.zeros((len(dtrain),1))\n",
        "    bins = [10,20,30,40,50,60]\n",
        "    arr,bins = np.histogram(dtrain,bins=bins)\n",
        "    weight_bins = max(arr)/arr\n",
        "    j=0\n",
        "    for i in dtrain:\n",
        "        if (10 <= i < 20):\n",
        "            weights[j] = weight_bins[0]\n",
        "        elif (20 <= i < 30):\n",
        "            weights[j] = weight_bins[1]\n",
        "        elif (30 <= i < 40):\n",
        "            weights[j] = weight_bins[2]\n",
        "        elif (40 <= i < 50):\n",
        "            weights[j] = weight_bins[3]\n",
        "        elif (50 <= i < 60):\n",
        "            weights[j] = weight_bins[4]\n",
        "        j = j + 1\n",
        "    weights = weights.reshape(len(weights),)\n",
        "    return weights"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfQ_gzYjT04h"
      },
      "source": [
        "def squared_err(dtrain,predt):\n",
        "    out = dtrain\n",
        "    weights = calcweights(dtrain)\n",
        "    grad = 2*weights*(predt - out)\n",
        "    hess = 2*weights\n",
        "    return grad, hess"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jvvbqq_UGl4"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVZeKT19TgbI"
      },
      "source": [
        "xgbst_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
        "             max_depth=8, min_child_weight=1,objective=squared_err, missing=None, n_estimators=160,\n",
        "             n_jobs=1, nthread=None, random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1.1, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SptVAKBMURXR",
        "outputId": "8f139d4d-0944-495c-d2cd-2a9c773d4776"
      },
      "source": [
        "#train the model on the train set\n",
        "xgbst_model.fit(X_train,y_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05:46:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(max_depth=8, n_estimators=160,\n",
              "             objective=<function squared_err at 0x7fdd797619e0>,\n",
              "             reg_lambda=1.1)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo8UtAEdUUBQ"
      },
      "source": [
        "#Predict the output on both test & train set\n",
        "\n",
        "yhat_xgbstval = xgbst_model.predict(X_test)\n",
        "\n",
        "yhat_xgbsttrain = xgbst_model.predict(X_train)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUTGkgkAUV6Q",
        "outputId": "d1107cea-9f9d-4311-e1f4-f4966fdf6b51"
      },
      "source": [
        "#Predict the r2Score for both test & train\n",
        "print(\"The score on the test set is:\",r2_score(y_test,yhat_xgbstval)*100)\n",
        "\n",
        "print(\"The score on the train set is:\",r2_score(y_train,yhat_xgbsttrain)*100)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score on the test set is: 45.558302006494046\n",
            "The score on the train set is: 62.452582019181314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbDJoyKaUf5x"
      },
      "source": [
        "### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj0v4Lq9Uhjw"
      },
      "source": [
        "lgbmodel = LGBMRegressor(learning_rate=0.1,objective=squared_err,max_depth=12,n_estimators=600)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Pb49DFUlpQ",
        "outputId": "a2eae81a-de42-49aa-83eb-e89ed8785e36"
      },
      "source": [
        "#Train the model on the train set\n",
        "lgbmodel.fit(X_train,y_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMRegressor(max_depth=12, n_estimators=600,\n",
              "              objective=<function squared_err at 0x7fdd797619e0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xWaSOrTUp7A"
      },
      "source": [
        "#Predict the output on both test and train set\n",
        "\n",
        "ylgb_val = lgbmodel.predict(X_test)\n",
        "\n",
        "ylgb_train = lgbmodel.predict(X_train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLTE8y5IUtuQ",
        "outputId": "021f3136-b582-4de0-db45-0601c09af68b"
      },
      "source": [
        "#Predict the r2Score for both test & train\n",
        "print(\"The score on the test set is:\",r2_score(y_test,ylgb_val)*100)\n",
        "\n",
        "print(\"The score on the train set is:\",r2_score(y_train,ylgb_train)*100)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score on the test set is: 46.051305620513986\n",
            "The score on the train set is: 62.056933549551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EIoX40gUz9A"
      },
      "source": [
        "## Stacking Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXlO_h0GU2hQ"
      },
      "source": [
        "In addition to previous method(See StackingRegressor.ipynb file), i have included two more models defined above in first level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dq3xGQCVFlf"
      },
      "source": [
        "#Add the models used in first & second level\n",
        "level0 = list()\n",
        "level0.append((\n",
        "    'xgbst',\n",
        "    XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
        "             max_depth=6, min_child_weight=1, missing=None, n_estimators=100,\n",
        "             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)\n",
        "    ))\n",
        "level0.append((\n",
        "    'histbst',\n",
        "    HistGradientBoostingRegressor(learning_rate=0.09)\n",
        "    ))\n",
        "level0.append((\n",
        "    'lgbbst',\n",
        "    LGBMRegressor(learning_rate=0.09)\n",
        "    ))\n",
        "level0.append((\n",
        "    'lgbweighted',\n",
        "    LGBMRegressor(learning_rate=0.1,objective=squared_err,max_depth=12,n_estimators=600)\n",
        "))\n",
        "level0.append((\n",
        "    'xgbstweighted',\n",
        "    XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
        "             max_depth=8, min_child_weight=1,objective=squared_err, missing=None, n_estimators=160,\n",
        "             n_jobs=1, nthread=None, random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1.1, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)\n",
        "))\n",
        "\n",
        "level1 = LGBMRegressor()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuQz-yPXYvOZ"
      },
      "source": [
        "#Build the stacking regressor model\n",
        "meta_model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIKU50aKYx5X",
        "outputId": "bd85d39a-3c07-4b84-c7bc-228a3f837a6c"
      },
      "source": [
        "#Train the model\n",
        "meta_model.fit(X_train,y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[05:52:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[05:58:47] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:00:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:01:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:02:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[06:04:07] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StackingRegressor(cv=5,\n",
              "                  estimators=[('xgbst',\n",
              "                               XGBRegressor(max_depth=6,\n",
              "                                            objective='reg:squarederror')),\n",
              "                              ('histbst',\n",
              "                               HistGradientBoostingRegressor(learning_rate=0.09)),\n",
              "                              ('lgbbst', LGBMRegressor(learning_rate=0.09)),\n",
              "                              ('lgbweighted',\n",
              "                               LGBMRegressor(max_depth=12, n_estimators=600,\n",
              "                                             objective=<function squared_err at 0x7fdd797619e0>)),\n",
              "                              ('xgbstweighted',\n",
              "                               XGBRegressor(max_depth=8, n_estimators=160,\n",
              "                                            objective=<function squared_err at 0x7fdd797619e0>,\n",
              "                                            reg_lambda=1.1))],\n",
              "                  final_estimator=LGBMRegressor())"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA4-XLkPY964"
      },
      "source": [
        "#Predict the target on validation & train set\n",
        "y_pred = meta_model.predict(X_test)\n",
        "\n",
        "ylrtrain_pred = meta_model.predict(X_train)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE7Rh7JP_1Pr",
        "outputId": "3824f378-2901-46a7-c6b0-505d2b5c6234"
      },
      "source": [
        "#Predict the r2Score for both test/validation & train set\n",
        "print(\"The score on the test set is:\",r2_score(y_test,y_pred)*100)\n",
        "\n",
        "print(\"The score on the train set is:\",r2_score(y_train,ylrtrain_pred)*100)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The score on the test set is: 53.21930585159122\n",
            "The score on the train set is: 55.677070395575804\n"
          ]
        }
      ]
    }
  ]
}